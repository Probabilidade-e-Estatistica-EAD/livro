Alguns pesquisadores podem argumentar a favor de um teste unilateral aqui, onde a alternativa consideraria apenas se o novo medicamento funciona melhor do que o medicamento padrão. No entanto, seria muito informativo saber se o novo medicamento tem um desempenho pior do que o medicamento padrão, por isso, usamos um teste bilateral para considerar essa possibilidade durante a análise.
```{example}
Os pesquisadores gostariam de realizar o ensaio clínico em pacientes com pressão arterial sistólica entre 140 e 180 mmHg. Suponha que estudos publicados anteriormente sugiram que o desvio padrão das pressões sanguíneas dos pacientes será de aproximadamente 12 mmHg e a distribuição das pressões sanguíneas dos pacientes será aproximadamente simétrica.^[Neste estudo em particular, geralmente medimos a pressão arterial de cada paciente no início e no final do estudo e, em seguida, a medição do resultado do estudo seria a variação média da pressão arterial. Isto é, ambos $\mu_{trmt}$ and $\mu_{ctrl}$ representaria diferenças médias. Isto é o que você pode pensar como uma estrutura de teste emparelhada de 2 amostras, e nós a analisaríamos exatamente como um teste de hipótese para uma diferença na mudança média para os pacientes. Nos cálculos que realizamos aqui, vamos supor que 12 ~mmHg é o desvio padrão previsto da diferença de pressão arterial de um paciente ao longo do estudo.] Se tivéssemos 100 pacientes por grupo, qual seria o erro padrão aproximado para $\bar{x}_{trmt} - \bar{x}_{ctrl}$?
```
O erro padrão é calculado da seguinte forma:
\begin{align*}
EP_{\bar{x}_{trmt} - \bar{x}_{ctrl}}
= \sqrt{\frac{s_{trmt}^2}{n_{trmt}} + \frac{s_{ctrl}^2}{n_{ctrl}}}
= \sqrt{\frac{12^2}{100} + \frac{12^2}{100}}
= 1.70
\end{align*}
Esta pode ser uma estimativa imperfeita do $EP_{\bar{x}_{trmt} - \bar{x}_{ctrl}}$, já que a estimativa do desvio padrão que usamos pode não ser correta para esse grupo de pacientes. No entanto, é suficiente para nossos propósitos.
```{example}
Com o que a distribuição nula de $\bar{x}_{trmt} - \bar{x}_{ctrl}$ se parece?
```
Os graus de liberdade são maiores que 30, então a distribuição de $\bar{x}_{trmt} - \bar{x}_{ctrl}$ será aproximadamente normal. O desvio padrão dessa distribuição (o erro padrão) seria de cerca de 1,70 e, sob a hipótese nula, sua média seria 0.
```{r, out.height='20%', fig.height=2}
set.seed(1)
X <- seq(-10, 10, 0.01)
Y <- dnorm(X, sd = 1.70)
gg   <- data.frame(X,Y)
ggplot(data = gg, mapping = aes(x = X, y = Y)) +
geom_hline(yintercept = 0, color = "white", size = 1) +
geom_path(color = '#E97C31', size = 1) +
labs(x  = NULL) +
theme(axis.title.y = element_blank(), axis.text.y = element_blank(),
axis.ticks.y = element_blank()) +
scale_x_continuous(breaks = seq(-9, 9, 3)) +
geom_vline(xintercept = 0, linetype = 'dashed', color = '#E97C31') +
annotate(geom = "text", x = 0, y = 0.01, label = expression(bar(x)[trmt] - bar(x)[ctrl]),
size = 4) +
annotate(geom = "text", x = 4, y = 0.2, label = 'Hipótese \nNula', size = 3, color = '#E97C31')+
theme(panel.border = element_rect(colour = "black", fill=NA, size=1))
```
```{example}
Para quais valores de $\bar{x}_{trmt} - \bar{x}_{ctrl}$ rejeitaríamos a hipótese nula?
```
Para $\alpha = 0.05$, nós rejeitaríamos $H_0$ se a diferença for na parte inferior 2.5\% ou superior 2.5\%:
__Inferior 2.5\%:__ Para o modelo normal, isso é 1.96 erros padrão abaixo de 0, portanto, qualquer diferença menor que $-1.96 \times 1.70 = -3.332$ mmHg.
__Superior 2.5\%:__ Para o modelo normal, isso é 1.96 erros padrão acima de 0, portanto, qualquer diferença maior que $1.96 \times 1.70 = 3.332$ mmHg.
Os limites dessas _regiões de rejeição_ são mostrados abaixo:
```{r, out.height = '20%', fig.height = 2}
ggplot(data = gg, mapping = aes(x = X, y = Y)) +
geom_hline(yintercept = 0, color = "white", size = 1) +
geom_path(color = '#E97C31', size = 1) +
labs(x  = NULL) +
theme(axis.title.y = element_blank(), axis.text.y = element_blank(),
axis.ticks.y = element_blank()) +
scale_x_continuous(breaks = seq(-9, 9, 3)) +
geom_vline(xintercept = 0, linetype = 'dashed', color = '#E97C31') +
annotate(geom = "text", x = 0, y = 0.01,
label = expression(bar(x)[trmt] - bar(x)[ctrl]), size = 4) +
annotate(geom = "text", x = 0, y = 0.1, label = 'Hipótese Nula \nNão rejeita H0',
size = 3, color = '#E97C31') +
geom_vline(xintercept = 3.332, linetype = 'dashed', color = 'black') +
geom_vline(xintercept = -3.332, linetype = 'dashed', color = 'black') +
annotate(geom = "text", x = 5, y = 0.1, label = 'Rejeita H0', size = 3, color = 'black') +
annotate(geom = "text", x = -5, y = 0.1, label = 'Rejeita H0', size = 3, color = 'black')+
theme(panel.border = element_rect(colour = "black", fill=NA, size=1))
```
Em seguida, realizaremos alguns cálculos hipotéticos para determinar a probabilidade de rejeitarmos a hipótese nula, se as hipóteses alternativas fossem realmente verdadeiras.
### Calculando a energia para um teste de 2 amostras
Ao planejar um estudo, queremos saber a probabilidade de detectarmos um efeito que nos interessa. Em outras palavras, se houver um efeito real, e esse efeito for grande o suficiente para ter valor prático, qual é a probabilidade de detectarmos esse efeito? Esta probabilidade é chamada de _poder_, e podemos calculá-lo para diferentes tamanhos de amostra ou para diferentes _tamanhos efetivos_.
Primeiro determinamos o que é um resultado praticamente significativo. Suponha que os pesquisadores da empresa se preocupem em encontrar qualquer efeito sobre a pressão arterial que seja de 3~mmHg ou maior em relação à medicação padrão. Aqui, 3mmHg é o mínimo _tamanho efetivo_ de interesse, e queremos saber qual a probabilidade de detectarmos esse tamanho efetivo no estudo.
```{example, label = 'PowerFor100AtNeg3'}
Suponha que decidimos prosseguir com 100 pacientes por grupo de tratamento e a nova droga reduz a pressão arterial em mais de 3mmHg em relação à medicação padrão. Qual é a probabilidade de detectarmos uma queda?
```
Antes mesmo de fazer qualquer cálculo, observe que se $\bar{x}_{trmt} - \bar{x}_{ctrl} = -3$ mmHg, não haveria provas suficientes para rejeitar $H_0$. Isso não é um bom sinal.
Para calcular a probabilidade de rejeitarmos $H_0$, precisamos determinar algumas coisas:
+ A distribuição amostral para $\bar{x}_{trmt} - \bar{x}_{ctrl}$ quando a verdadeira diferença é -3~mmHg. Isto é o mesmo que a distribuição nula, exceto que é deslocado para a esquerda por 3:
```{r, out.height='20%', fig.height=2}
set.seed(1)
ggplot() +
geom_hline(yintercept = 0, color = "white", size = 1) +
geom_path(data = data.frame(X = seq(-10, 10, 0.01),Y = dnorm(X, sd = 1.70)),
mapping = aes(x = X, y = Y), color = '#E97C31', size = 1) +
geom_path(data = data.frame(X = seq(-10, 10, 0.01),Y = dnorm(X, mean = -3, sd = 1.70)),
mapping = aes(x = X, y = Y), color = '#EAB217', size = 1) +
theme(axis.title.y = element_blank(), axis.text.y = element_blank(),
axis.ticks.y = element_blank()) +
scale_x_continuous(breaks = seq(-9, 9, 3)) +
geom_vline(xintercept = -3, linetype = 'dashed', color = '#EAB217') +
geom_vline(xintercept = 0, linetype = 'dashed', color = '#E97C31') +
labs(x  = NULL) +
annotate(geom = "text", x = 0, y = 0.01,
label = expression(bar(x)[trmt] - bar(x)[ctrl]), size = 4) +
annotate(geom = "text", x = 2.3, y = 0.2, label = 'Hipótese \nNula', size = 3, color = '#E97C31') +
annotate(geom = "text", x = -6, y = 0.2, label = c("Distribuição com \n",
expression(mu[trmt] - mu[ctrl]*" = -3")), size = 3, color = '#EAB217') +
geom_vline(xintercept = 3.332, linetype = 'dashed', color = 'black') +
geom_vline(xintercept = -3.332, linetype = 'dashed', color = 'black')+
theme(panel.border = element_rect(colour = "black", fill=NA, size=1))
```
+ As regiões de rejeição, que estão fora das linhas pontilhadas acima.
+ A fração da distribuição que cai na região de rejeição.
Em suma, precisamos calcular a probabilidade de que $x < -3.332$ para uma distribuição normal com média -3 e desvio padrão 1.7. Para fazer isso, primeiro sombreamos a área que queremos calcular:
```{r, out.height = '20%', fig.height = 2}
set.seed(1)
gg = data.frame(X = seq(-10, 10, 0.01),Y = dnorm(X, mean = -3, sd = 1.70))
ggplot() +
geom_linerange(data = gg[gg$X < -3.332,], aes(X, ymin = -0.003, ymax = Y),
colour="palegoldenrod") +
geom_path(data = data.frame(X = seq(-10, 10, 0.01),Y = dnorm(X, sd = 1.70)),
mapping = aes(x = X, y = Y), color = '#E97C31', size = 1) +
geom_path(data = gg,
mapping = aes(x = X, y = Y), color = '#EAB217', size = 1) +
theme(axis.title.y = element_blank(), axis.text.y = element_blank(),
axis.ticks.y = element_blank()) +
scale_x_continuous(breaks = seq(-9, 9, 3)) +
geom_hline(yintercept = -0.003, color = 'white', size = 1) +
geom_vline(xintercept = -3, linetype = 'dashed', color = '#EAB217') +
geom_vline(xintercept = 0, linetype = 'dashed', color = '#E97C31') +
labs(x  = NULL) +
annotate(geom = "text", x = 0, y = 0.01,
label = expression(bar(x)[trmt] - bar(x)[ctrl]), size = 4) +
annotate(geom = "text", x = 1.5, y = 0.05, label = 'Hipótese \nNula',
size = 3, color = '#E97C31') +
annotate(geom = "text", x = -8.5, y = 0.05, label = c("Distribuição com \n",
expression(mu[trmt] - mu[ctrl]*" = -3")), size = 3, color = '#EAB217') +
geom_vline(xintercept = 3.332, linetype = 'dashed', color = 'black') +
geom_vline(xintercept = -3.332, linetype = 'dashed', color = 'black') +
theme(panel.border = element_rect(colour = "black", fill=NA, size=1))
```
Então, calculamos o escore Z e encontramos a área da cauda usando a tabela de probabilidades normal ou o software estatístico:
\begin{align*}
Z = \frac{-3.332 - (-3)}{1.7} = -0.20 \qquad \to \qquad 0.4207
\end{align*}
O poder para o teste é 42\% quando $\mu_{trmt} - \mu_{ctrl} = -3$ e cada grupo tem um tamanho de amostra de 100.
No Exemplo \@ref(exm:PowerFor100AtNeg3), ignoramos a região de rejeição superior no cálculo, que estava na direção oposta da verdade hipotética, ou seja, -3. O raciocínio? Não haveria nenhum valor em rejeitar a hipótese nula e concluir que houve um aumento quando de fato houve uma diminuição.
### Determinando um tamanho de amostra adequado
o último exemplo, descobrimos que, se temos um tamanho de amostra de 100 em cada grupo, só podemos detectar um tamanho efetivo de 3~mmHg com uma probabilidade de cerca de 0,42. Suponha que os pesquisadores avançassem e usassem apenas 100 pacientes por grupo, e os dados não sustentaram a hipótese alternativa, ou seja, os pesquisadores não rejeitaram $H_0$. Esta é uma situação muito ruim por algumas razões:
+ Na parte de trás da mente dos pesquisadores, todos estariam se perguntando, _talvez haja uma diferença real e significativa, mas não conseguimos detectá-lo com uma amostra tão pequena_.
+ A empresa provavelmente investiu centenas de milhões de dólares no desenvolvimento do novo medicamento, então agora eles ficam com uma grande incerteza sobre seu potencial, já que o experimento não teve uma grande chance de detectar efeitos que ainda poderiam ser importantes.
+ Os pacientes foram submetidos à droga, e não podemos dizer com muita certeza que a droga não ajuda (ou prejudica) os pacientes.
+ Outro ensaio clínico pode precisar ser executado para obter uma resposta mais conclusiva sobre se o medicamento possui algum valor prático, e a realização de um segundo teste clínico pode levar anos e muitos milhões de dólares.
Queremos evitar essa situação, por isso precisamos determinar um tamanho de amostra apropriado para garantir que possamos ter certeza de que detectaremos quaisquer efeitos que sejam importantes. Como mencionado anteriormente, uma mudança de 3~mmHg foi considerada a diferença mínima que era importante. Como primeiro passo, poderíamos calcular a potência para vários tamanhos de amostras diferentes. Por exemplo, vamos tentar 500 pacientes por grupo.
***
```{exercise}
Calcule o poder de detectar uma mudança de -3 mmHg ao usar um tamanho de amostra de 500 por grupo.^[(a) O erro padrão é dado como $EP = \sqrt{\frac{12^2}{500} + \frac{12^2}{500}} = 0.76$.\\
(b) \& (c) A distribuição nula, os limites de rejeição e a distribuição alternativa devem ser desenhados. As regiões de rejeição estão a $\pm 0.76 \times 1.96 = \pm 1.49$. (d) A área da distribuição alternativa, onde $\mu_{trmt} - \mu_{ctrl} = -3$ deve ser sombreada. O Z-escore para a área da cauda: $Z = \frac{-1.49 - (-3)}{0.76} = 1.99 \to 0.9767$. Com 500 pacientes por grupo, teríamos cerca de 97,7\% de certeza (ou mais) de que detectamos quaisquer efeitos que tenham pelo menos 3 mmHg de tamanho.]
+ (a). Determine o erro padrão (lembre-se que o desvio padrão dos pacientes deveria ser de cerca de 12mmHg).
+ (b). Identifica as regiões de distribuição e rejeição nula.
+ (c). Identifique a distribuição alternativa quando $\mu_{trmt} - \mu_{ctrl} = -3$.
+ (d). Calcule a probabilidade de rejeitarmos a hipótese nula.
```
***
Os pesquisadores decidiram que 3 mmHg foi a diferença mínima que era importante, e com um tamanho de amostra de 500, podemos estar muito certos (97,7\% ou melhor) de que vamos detectar qualquer diferença. Passamos agora para outro extremo em que estamos expondo um número desnecessário de pacientes ao novo medicamento no ensaio clínico. Isso não só é eticamente questionável, como também custaria muito mais dinheiro do que o necessário para ter certeza de que detectaríamos quaisquer efeitos importantes.
A prática mais comum é identificar o tamanho da amostra em que a potência está em torno de 80\% e, às vezes, 90\%. Outros valores podem ser razoáveis para um contexto específico, mas 80\% e 90\% são mais comumente direcionados como um bom equilíbrio entre alta potência e não expor muitos pacientes a um novo tratamento (ou desperdiçar muito dinheiro). Poderíamos calcular o poder do teste em vários outros tamanhos de amostra possíveis até encontrarmos um que esteja próximo de 80\%, mas isso é ineficiente. Em vez disso, devemos resolver o problema de trás para frente.
```{example}
Qual o tamanho da amostra levará a um poder de 80\%?
```
Começamos por identificar o escore Z que nos daria uma cauda inferior de 80 \%: seria cerca de 0.84:
```{r, out.height='20%', fig.height=2}
set.seed(1)
gg = data.frame(X = seq(-10, 10, 0.01),Y = dnorm(X, mean = -3, sd = 1.70))
ggplot() +
geom_linerange(data = gg[gg$X < -3+(0.84),],
aes(X, ymin = -0.003, ymax = Y), colour="palegoldenrod")  +
geom_hline(yintercept = -0.003, color = 'white', size = 1) +
geom_path(data = data.frame(X = seq(-10, 10, 0.01),Y = dnorm(X, sd = 1.70)),
mapping = aes(x = X, y = Y), color = '#E97C31', size = 1) +
geom_path(data = gg, mapping = aes(x = X, y = Y), color = '#EAB217', size = 1) +
theme(axis.title.y = element_blank(), axis.text.y = element_blank(),
axis.ticks.y = element_blank()) +
scale_x_continuous(breaks = seq(-9, 9, 3)) +
geom_vline(xintercept = -3, linetype = 'dashed', color = '#EAB217') +
geom_vline(xintercept = 0, linetype = 'dashed', color = '#E97C31') +
labs(x  = NULL) +
annotate(geom = "text", x = 1.5, y = 0.05,
label = 'Hipótese \nNula', size = 3, color = '#E97C31') +
annotate(geom = "text", x = -8.5, y = 0.05, label = c("Distribuição com \n",
expression(mu[trmt] - mu[ctrl]*" = -3")), size = 3, color = '#EAB217') +
geom_vline(xintercept = 3.332, linetype = 'dashed', color = 'black') +
geom_vline(xintercept = -3.332, linetype = 'dashed', color = 'black')+
theme(panel.border = element_rect(colour = "black", fill=NA, size=1))
```
Além disso, a região de rejeição sempre se estende $1.96\times EP$ do centro da distribuição nula para $\alpha = 0.05$. Isso nos permite calcular a distância alvo entre o centro das distribuições nula e alternativa em termos do erro padrão:
\begin{align*}
0.84 \times EP + 1.96 \times EP = 2.8 \times EP
\end{align*}
Em nosso exemplo, também queremos que a distância entre os centros de distribuição nula e alternativa seja igual ao tamanho efetivo mínimo de interesse, 3~mmHg, que nos permite configurar uma equação entre essa diferença e o erro padrão:
\begin{align*}
3 &= 2.8 \times EP \\
3 &= 2.8 \times \sqrt{\frac{12^2}{n} + \frac{12^2}{n}} \\
% 3^2 &= 2.8^2 \times \left( \frac{12^2}{n} + \frac{12^2}{n} \right) \\
n &= \frac{2.8^2}{3^2} \times \left( 12^2 + 12^2 \right) = 250.88 \\
\end{align*}
Devemos visar cerca de 251 pacientes por grupo.
A diferença de erro padrão de $2.8 \times EP$ é específico para um contexto em que a potência alvo é de 80\% e o nível de significância é $\alpha = 0.05$. Se a potência desejada é de 90\% ou se usarmos um nível de significância diferente, usaremos algo um pouco diferente $2.8 \times EP$.
***
```{exercise}
Suponha que a potência fosse $90\%$ e estivéssemos usando $\alpha = 0.01$. Quantos erros-padrão devem separar os centros da distribuição nula e alternativa, onde a distribuição alternativa é centralizada no tamanho efetivo mínimo de interesse? Suponha que o teste tenha dois lados.^[Primeiro, encontre a pontuação Z de tal forma que $90\%$ da distribuição esteja abaixo dela: $Z = 1.28$. Em seguida, encontre os pontos de corte para as regiões de rejeição: $\pm 2.58$. Então a diferença nos centros deve ser sobre $1.28 \times EP + 2.58 \times EP = 3.86 \times EP$.]
```
***
***
```{exercise}
Liste algumas considerações que são importantes para determinar qual deve ser o poder de um experimento.^[As respostas vão variar, mas aqui estão algumas considerações importantes: se existe algum risco para os pacientes no estudo. O custo de inscrição de mais pacientes. A desvantagem potencial de não detectar um efeito de interesse.]
```
***
A Figura \@ref(fig:power_curve_neg-3) mostra o poder para tamanhos de amostra de 20 a 5.000 pacientes quando $\alpha = 0.05$ e a verdadeira diferença é -3. Esta curva foi construída escrevendo um programa para calcular a potência para muitos tamanhos de amostras diferentes.
```{r power_curve_neg-3, fig.cap='A curva mostra a potência para diferentes tamanhos de amostra no contexto do exemplo da pressão arterial quando a diferença real é -3. Ter mais de cerca de 250 a 350 observações não fornece muito valor adicional na detecção de um efeito quando alfa = 0.05.'}
```
Cálculos de energia para experimentos caros ou arriscados são críticos. No entanto, o que acontece com experimentos que são baratos e onde as considerações éticas são mínimas? Por exemplo, se estamos realizando testes finais em um novo recurso em um site popular, como as nossas considerações sobre tamanho de amostra mudariam? Como antes, queremos ter certeza de que a amostra é grande o suficiente. No entanto, se o recurso foi submetido a alguns testes e sabe-se que funciona bem (ou seja, não frustra muitos usuários do site), podemos executar um experimento muito maior do que o necessário para detectar os efeitos mínimos de interesse. A razão é que pode haver benefícios adicionais em ter uma estimativa ainda mais precisa do efeito do novo recurso. Podemos até realizar um grande experimento como parte do lançamento do novo recurso.
## Comparando muitas médias com ANOVA (tópico especial)
Às vezes, queremos comparar médias em muitos grupos. Poderíamos inicialmente pensar em fazer comparações pareadas; por exemplo, se houvesse três grupos, poderíamos ser tentados a comparar a primeira média com a segunda, depois com a terceira e, finalmente, comparar a segunda e a terceira médias para um total de três comparações. No entanto, essa estratégia pode ser traiçoeira. Se tivermos muitos grupos e fizermos muitas comparações, é provável que acabemos por encontrar uma diferença apenas por acaso, mesmo que não haja diferença nas populações.
Nesta seção, vamos aprender um novo método chamado \term{análise de variância (ANOVA)} e uma nova estatística de teste chamada $F$. ANOVA usa um teste de hipótese único para verificar se as médias em muitos grupos são iguais:
\begin{cases}
H_0: \mbox{O resultado médio é o mesmo em todos os grupos. Na notação estatística, }\mu_1 = \mu_2 = \cdots = \mu_k \\
H_1: \mbox{Pelo menos uma média é diferente.}
\end{cases}
onde $\mu_i$ representa a média do resultado para observações na categoria i.
Geralmente, devemos verificar três condições nos dados antes de executar a ANOVA:
+ as observações são independentes dentro e entre grupos,
+ os dados dentro de cada grupo são quase normais, e
+ a variabilidade entre os grupos é aproximadamente igual.
Quando essas três condições forem satisfeitas, podemos executar uma ANOVA para determinar se os dados fornecem evidências fortes contra a hipótese nula de que $\mu_i$ são iguais.
```{example, label = 'firstExampleForThreeStatisticsClassesAndANOVA'}
Os departamentos universitários geralmente realizam várias classes do mesmo curso introdutório a cada semestre por causa da alta demanda. Considere um departamento de estatística que executa três classes de um curso de estatística introdutória. Poderíamos determinar se há diferenças estatisticamente significativas nas pontuações do primeiro exame nessas três classes ($A$, $B$, e $C$). Descrever hipóteses apropriadas para determinar se existem diferenças entre as três classes.
```
As hipóteses podem ser escritas da seguinte forma:
+ $H_0$: A pontuação média é idêntica em todas as classes. Qualquer diferença observada é devida ao acaso. Notoriamente, nós escrevemos $\mu_A=\mu_B=\mu_C$.
+ $H_1$: A pontuação média varia de acordo com a classe. Iríamos rejeitar a hipótese nula em favor da hipótese alternativa se houvesse maiores diferenças entre as médias de classe do que o que poderíamos esperar do acaso sozinho.
Fortes evidências favorecendo a hipótese alternativa na ANOVA são descritas por diferenças incomumente grandes entre as médias do grupo. Em breve aprenderemos que avaliar a variabilidade das médias do grupo em relação à variabilidade entre observações individuais dentro de cada grupo é fundamental para o sucesso da ANOVA.
```{example}
Examine a Figura \@ref(fig:toyANOVA). Compare os grupos I, II e III. Você pode determinar visualmente se as diferenças nos centros de grupo são devidas ao acaso ou não? Agora compare os grupos IV, V e VI. Essas diferenças parecem ser devidas ao acaso?
```
Qualquer diferença real nas médias dos grupos I, II e III é difícil de discernir, porque os dados dentro de cada grupo são muito voláteis em relação a quaisquer diferenças no resultado médio. Por outro lado, parece haver diferenças nos centros dos grupos IV, V e VI. Por exemplo, o grupo V parece ter uma média maior que a dos outros dois grupos. Investigando os grupos IV, V e VI, vemos que as diferenças nos centros dos grupos são perceptíveis, porque essas diferenças são grandes _em relação à variabilidade nas observações individuais dentro de cada grupo_.
```{r toyANOVA, fig.cap = "Gráfico de pontos lado a lado para os resultados de seis grupos."}
gps <- c("I", "II", "III")
g <- as.factor(rep(gps, c(20, 10, 40)))
g2 <- as.factor(rep(c("IV", "V", "VI"), c(20, 10, 40)))
M <- c(1, 2, 1.5)
names(M) <- gps
set.seed(7)
X1 <- rnorm(length(g), M[g], 1.5)
X2 <- rnorm(length(g), M[g], 0.5)
ggplot() +
geom_point(aes(x = g, y = X1), alpha = 0.5, color = '#E6205F') +
geom_point(aes(x = g2, y = X2), alpha = 0.5, color = '#EAB217') +
labs(x = NULL, y = 'Outcome') +
theme(panel.border = element_rect(colour = "black", fill=NA, size=1))
```
### O desempenho de rebatidas está relacionado à posição do jogador na MLB?
Gostaríamos de discernir se há diferenças reais entre o desempenho de rebatedores de jogadores de beisebol de acordo com sua posição: outfielder (OF), infielder (IF), designated hitter (DH), e apanhador (A). Vamos usar um conjunto de dados chamado `bat10`, que inclui registros de rebatidas de 327 jogadores da Major League Baseball (MLB) da temporada de 2010. Seis dos 327 casos representados em `bat10` são mostrados na Tabela \@ref(tab:mlbBat10DataMatrix), e descrições para cada variável são fornecidas em Tabela \@ref(tab:mlbBat10Variables). A medida que usaremos para o desempenho de rebatimento do jogador (a variável de resultado) é a porcentagem básica __OBP__. A porcentagem na base representa aproximadamente a fração do tempo que um jogador obtém com sucesso na base ou atinge um home run.
```{r mlbBat10DataMatrix}
data(mlbBat10)
knitr::kable(head(mlbBat10), align = "c",
caption = "Seis casos da matriz de dados bat10.")
```
```{r mlbBat10Variables}
res <- matrix(NA, ncol = 2, nrow = 9)
res[,1] <- c('nome', 'time', 'posição', 'AB', 'H', 'HR', 'RBI', 'AVG', 'OBP')
res[,2] <- c('Nome do jogador',
'O nome abreviado da equipe do jogador',
'A posição do campo principal do jogador (OF, IF, DH, A)',
'Número de oportunidades no taco',
'Número de batidas',
'Número de home runs',
'Número de corridas em batidas',
'Média de rebatidas, que é igual a H/AB',
'Porcentagem na base, que é aproximadamente igual à fração de vezes que um jogador fica na base ou atinge um home run')
colnames(res) <- c('variável','descrição')
knitr::kable(res, align = "c",
caption = "Variáveis e suas descrições para o conjunto de dados bat10.")
```
***
```{exercise}
A hipótese nula sob consideração é a seguinte: $\mu_{OF} = \mu_{IF} = \mu_{DH} = \mu_{A}$.
Escreva as hipóteses alternativas nula e correspondente em linguagem simples.^[$H_0$: A porcentagem média na base é igual entre as quatro posições. $H_1$: A porcentagem média na base varia em alguns (ou todos) grupos.]
```
***
```{example}
As posições dos jogadores foram divididas em quatro grupos: outfield (OF), infield (IF), designated hitter (DH), e apanhador (A). Qual seria uma estimativa pontual apropriada da porcentagem na base por outfielders, $\mu_{OF}$?}
Uma boa estimativa da porcentagem em bases por outfielders seria a média da amostra de \var{OBP} para apenas aqueles jogadores cuja posição é outfield: $\bar{x}_{OF} = 0.334$.
```
A Tabela \@ref(tab:mlbHRPerABSummaryTable) fornece estatísticas resumidas para cada grupo. Um gráfico de caixa lado-a-lado para a porcentagem na base é mostrado na Figura \@ref(fig:mlbANOVABoxPlot). Observe que a variabilidade parece ser aproximadamente constante entre os grupos; a variação quase constante entre os grupos é uma suposição importante que deve ser satisfeita antes de considerarmos a abordagem ANOVA.
```{r mlbHRPerABSummaryTable}
desc <- matrix(NA, ncol = 5, nrow = 3)
desc[1,] <- c('Tamanho da amostra', 120, 154, 14, 39)
desc[2,] <- c('Média da amostra', 0.334, 0.332, 0.348, 0.323)
desc[3,] <- c('DP amostral', 0.029, 0.037, 0.036, 0.045)
colnames(desc) <- c('','OF', 'IF', 'DH', 'A')
knitr::kable(desc, align = "c",
caption = "Estatísticas resumidas da porcentagem na base, dividida pela posição do jogador")
```
```{r mlbANOVABoxPlot, fig.cap='Gráfico de boxplot da porcentagem na base para 327 jogadores em quatro grupos. Há um outlier proeminente visível no grupo infield, mas com 154 observações no grupo infield, esse outlier não é uma preocupação.'}
library(openintro)
data(COL)
data(mlbBat10)
d   <- mlbBat10[mlbBat10$AB > 200,]
pos <- list(c("OF"), c("1B", "2B", "3B", "SS"), "DH", "C")
POS <- c("OF", "IF", "DH", "C")
out <- c()
gp  <- c()
for (i in 1:length(pos)) {
these <- which(d$pos %in% pos[[i]])
out   <- c(out, d[these,"OBP"])
gp    <- c(gp, rep(POS[i], length(these)))
}
DF  <- data.frame(d, gp = gp)
ggplot() +
geom_boxplot(aes(x = gp, y = out, fill = gp)) +
theme(legend.position = "none") +
labs(x = 'Posição', y = 'Porcentagem na base') +
theme(panel.border = element_rect(colour = "black", fill=NA, size=1)) +
scale_y_continuous(labels = scales::percent_format())
```
```{example, label = "multipleComparisonExampleThatIncludesDiscussionOfClassrooms"}
A maior diferença entre as médias da amostra é entre o designated hitter e o apanhador. Considere novamente as hipóteses originais:
```
$$
\begin{cases}
H_0: & \mu_{OF} = \mu_{IF} = \mu_{DH} = \mu_{C} \\
H_1: & \mbox{A porcentagem média na base } (\mu_i) \mbox{ varia entre alguns (ou todos) grupos.}
\end{cases}
$$
Por que seria inadequado executar o teste simplesmente estimando se a diferença de $\mu_{{DH}$ e $\mu_{C}$ é estatisticamente significativa a um nível de significância de 0,05?
A principal questão aqui é que estamos inspecionando os dados antes de escolher os grupos que serão comparados. Não é apropriado examinar todos os dados a olho nu (testes informais) e só depois decidir quais partes testar formalmente. Isso é chamado __bisbilhotando dados__ ou __pescando dados__. Naturalmente, escolheríamos os grupos com as grandes diferenças para o teste formal, levando a uma inflação na taxa de erro tipo~1. Para entender isso melhor, vamos considerar um problema ligeiramente diferente.
Suponha que estamos a medir a aptidão para os alunos em 20 turmas de uma escola primária no início do ano. Nesta escola, todos os alunos são aleatoriamente designados para as salas de aula, portanto, quaisquer diferenças que observamos entre as classes no início do ano são completamente devidas ao acaso. No entanto, com tantos grupos, provavelmente observaremos alguns grupos que parecem bastante diferentes um do outro. Se selecionarmos apenas essas classes que parecem tão diferentes, provavelmente faremos a conclusão errada de que a tarefa não foi aleatória. Embora possamos apenas testar formalmente as diferenças para alguns pares de classes, avaliamos informalmente as outras classes de olho antes de escolher os casos mais extremos para uma comparação.
Para informações adicionais sobre as idéias expressas no Exemplo \@ref(exm:multipleComparisonExampleThatIncludesDiscussionOfClassrooms), eecomendamos ler sobre a __falácia do promotor__.
Na próxima seção, aprenderemos como usar a estatística $F$ e a ANOVA para testar se as diferenças observadas nas médias da amostra poderiam ter acontecido apenas por acaso, mesmo que não houvesse diferença nas respectivas médias populacionais.
### Análise de variância (ANOVA) e o teste F
O método de análise de variância neste contexto se concentra em responder a uma questão: a variabilidade na amostra é tão grande que parece improvável que seja apenas por acaso? Esta questão é diferente dos procedimentos de teste anteriores, uma vez que, _simultaneamente_, considera muitos grupos e avalia se suas médias amostrais diferem mais do que esperávamos da variação natural. Nós chamamos isso de variabilidade __média quadrada entre os grupos ($MSG$)__, e tem graus de liberdade associados, $GL_{G}=k-1$ quando há $k$ grupos. A $MSG$ pode ser pensado como uma fórmula de variância escalonada para médias. Se a hipótese nula for verdadeira, qualquer variação na amostra significa devido ao acaso e não deve ser muito grande. Detalhes dos cálculos de $MSG$ são fornecidos na nota de rodapé,^[$\bar{x}$ representam a média dos resultados em todos os grupos. Então, a MSG é computada como $MSG = \frac{1}{df_{G}}SSG = \frac{1}{k-1}\sum_{i=1}^{k} n_{i}\left(\bar{x}_{i} - \bar{x}\right)^2$, onde $SSG$ é chamada de __soma dos quadrados entre os grupos__ e $n_{i} $ é o tamanho da amostra do grupo $i$.] no entanto, normalmente usamos software para esses cálculos.
A MSG é, por si só, completamente inútil em um teste de hipótese. Precisamos de um valor de referência para a quantidade de variabilidade esperada entre as médias da amostra, se a hipótese nula for verdadeira. Para este fim, calculamos uma estimativa de variância agrupada, frequentemente abreviada como __erro quadrático médio ($MEP$)__, que tem um grau de liberdade associado $GL_E=n-k$. É útil pensar em $MEP$ como uma medida da variabilidade dentro dos grupos. Detalhes dos cálculos do $MEP$ são fornecidos na nota de rodapé^[$\bar{x}$ representam a média dos resultados em todos os grupos. Então a  __soma do total de quadrados ($SST$)__ é calculada como $SST = \sum_{i=1}^{n} \left(x_{i} - \bar{x}\right)^2$, onde a soma é sobre todas as observações no conjunto de dados. Então calculamos o __soma de erros quadrados ($SEP$)__ de duas maneiras equivalentes: $SEP &= SST - SSG = (n_1-1)s_1^2 + (n_2-1)s_2^2 + \cdots + (n_k-1)s_k^2$,onde $s_i^2$ é a variância da amostra (quadrado do desvio padrão) dos resíduos no grupo $i$. Então o $MEP$ é a forma padronizada do $SEP$: $MEP = \frac{1}{df_{E}}SEP$.] para leitores interessados.
Quando a hipótese nula é verdadeira, quaisquer diferenças entre as médias amostrais são devidas apenas ao acaso, e $MSG$ e $MEP$ devem ser aproximadamente iguais. Como estatística de teste para ANOVA, examinamos a fração de $MSG$ e $MEP$:
\begin{align}
F = \frac{MSG}{MEP}
(\#eq:formulaForTheFStatistic)
\end{align}
A $MSG$ representa uma medida da variabilidade entre grupos, e $MEP$ mede a variabilidade dentro de cada um dos grupos.
***
```{exercise}
Para os dados do basebol, $MSG = 0,00252$ e $MEP = 0,00127$. Identifique os graus de liberdade associados ao MSG e MEP e verifique se a estatística $F$ é aproximadamente 1.994.^[Há $k=4$ grupos, logo $GL_{G} = k-1 = 3$. Há $n = n_1 + n_2 + n_3 + n_4 = 327$ observações, logo $GL_{E} = n - k = 323$. Em seguida, a estatística $F$ é calculada como a proporção de $MSG$ e $MEP$: $F = \frac{MSG}{MEP} = \frac{0.00252}{0.00127} = 1.984 \approx 1.994$. ($F=1.994$ foi calculado usando valores para $MSG$ e $MEP$ que não foram arredondados.)]
```
***
Podemos usar a estatística $F$ para avaliar as hipóteses no que é chamado de __Teste F__. Um p-valor pode ser calculado a partir da estatística $F$ usando uma distribuição $F$, que possui dois parâmetros associados: $GL_{1}$ e $GL_{2}$. Para a estatística $F$ em ANOVA, $GL_{1} = GL_{G}$ e $GL_{2}= GL_{E}$. Uma distribuição $F$ com 3 e 323 graus de liberdade, correspondente à estatística $F$ para o teste de hipótese de beisebol, é mostrada na Figura \@ref(fig:fDist3And323).
```{r fDist3And323, fig.cap = "Uma  distribuição F com GL_1 = 3 e GL2 = 323."}
ggplot(data = data.frame(x = c(0, 6)), aes(x)) +
geom_hline(yintercept = -0.008, size = 1, color = "white") +
stat_function(fun = df, n = 1001, args = list(df1 = 3, df2 = 323), size = 1)  +
theme(axis.title.y = element_blank(), axis.text.y = element_blank(),
axis.ticks.y = element_blank()) + labs(x = NULL) +
scale_x_continuous(breaks = seq(0, 6, 1)) +
theme(panel.border = element_rect(colour = "black", fill=NA, size=1))
```
Quanto maior a variabilidade observada nas médias amostrais ($MSG$) em relação às observações dentro do grupo ($MEP$), maior será $F$ e mais forte será a evidência contra a hipótese nula. Como valores maiores de $F$ representam evidências mais fortes contra a hipótese nula, usamos a cauda superior da distribuição para calcular um p-valor.
<div class="alert alert-info">
<strong> A estatística $F$ e o teste $F$</strong>: A análise de variância (ANOVA) é usada para testar se o resultado médio difere em 2 ou mais grupos. A ANOVA usa uma estatística de teste $F$, que representa uma relação padronizada de variabilidade nas médias da amostra em relação à variabilidade dentro dos grupos. Se $H_0$ for verdadeiro e as suposições do modelo forem satisfeitas, a estatística $F$ segue uma distribuição $F$ com parâmetros $GL_{1}=k-1$ e $GL_{2}=n-k$. A cauda superior da distribuição $F$ é usada para representar o p-valor.
</div>
***
```{exercise, label = 'describePValueAreaForFDistributionInMLBOBPExample'}
A estatística de teste para o exemplo de basebol é $F=1.994$. Sombreie a área correspondente ao p-valor na Figura \@ref(fig:fDist3And323)
```
***
```{example}
O p-valor correspondente à área sombreada na solução de Prática Orientada \@ref(exr:describePValueAreaForFDistributionInMLBOBPExample) é igual a cerca de 0.115. Isso fornece fortes evidências contra a hipótese nula?
```
O p-valor é maior que 0.05, indicando que a evidência não é forte o suficiente para rejeitar a hipótese nula em um nível de significância de 0.05. Ou seja, os dados não fornecem evidências fortes de que a porcentagem média na base varia de acordo com a posição primária do jogador.
### Lendo uma tabela ANOVA do software
Os cálculos necessários para executar uma ANOVA à mão são tediosos e propensos a erros humanos. Por essas razões, é comum usar um software estatístico para calcular a estatística e o p-valor $F$.
Uma ANOVA pode ser resumida em uma tabela muito semelhante à de um resumo de regressão, que veremos mais adiante. A Tabela \@ref(tab:anovaSummaryTableForOBPAgainstPosition) mostra um resumo da ANOVA para testar se a média da porcentagem na base varia de acordo com as posições dos jogadores na MLB. Muitos desses valores devem parecer familiares; em particular, a estatística de teste $F$ e o p-valor podem ser recuperados das últimas colunas.
```{r anovaSummaryTableForOBPAgainstPosition}
mod <- lm(out ~ as.factor(gp))
knitr::kable(round(anova(mod), 4), align = "c",
caption = 'Resumo da ANOVA para testar se a porcentagem média na base difere entre as posições dos jogadores.')
```
###  Diagnóstico gráfico para uma análise ANOVA
Existem três condições que devemos verificar para uma análise de ANOVA: todas as observações devem ser independentes, os dados em cada grupo devem ser quase normais e a variação dentro de cada grupo deve ser aproximadamente igual.
+ __Independência__: Se os dados são uma amostra aleatória simples de menos de 10\% da população, esta condição é satisfeita. Para processos e experimentos, considere cuidadosamente se os dados podem ser independentes (por exemplo, sem pareamento). Por exemplo, nos dados da MLB, os dados não foram amostrados. No entanto, não existem razões óbvias pelas quais a independência não seria válida para a maioria ou para todas as observações.
+ __Aproximadamente normal__: Como no teste de uma e duas amostras para médias, a suposição de normalidade é especialmente importante quando o tamanho da amostra é muito pequeno. Os gráficos de probabilidade normal para cada grupo dos dados da MLB são mostrados na Figura \@ref(fig:mlbANOVADiagNormalityGroups); há algum desvio da normalidade para os que estão em campo, mas isso não é uma preocupação substancial, pois há cerca de 150 observações nesse grupo e os outliers não são extremos. Às vezes, na ANOVA, há tantos grupos ou tão poucas observações por grupo que verificar a normalidade para cada grupo não é razoável. Veja a nota de rodapé^[ Primeiro calcule os __resíduos__ dos dados do beisebol, que são calculados tomando os valores observados e subtraindo o grupo correspondente significa. Por exemplo, um outfielder com OBP de 0.405 teria um resíduo de $0.405 - \bar{x}_{OF} = 0.071$. Em seguida, para verificar a condição de normalidade, crie um gráfico de probabilidade normal usando todos os resíduos simultaneamente] para obter orientação sobre como lidar com tais instâncias.
```{r mlbANOVADiagNormalityGroups, fig.cap="Gráfico de probabilidade normal do OBP para cada posição de campo."}
require(qqplotr)
ggplot(data = data.frame(out, gp), aes(sample = out)) +
stat_qq(color = '#E6205F') +
labs(x = "Quantis Teóricos",y = "") + facet_wrap(~gp) +
theme(legend.position = "none") +
theme(panel.border = element_rect(colour = "black", fill = NA, size = 1))
```
+ __Variação constante__: A última suposição é que a variação nos grupos é aproximadamente igual de um grupo para o próximo. Esta suposição pode ser verificada examinando uma caixa lado a lado dos resultados entre os grupos, como na Figura \@ref(fig:mlbANOVABoxPlot). Nesse caso, a variabilidade é semelhante nos quatro grupos, mas não é idêntica. Nós vemos na Tabela \@ref(tab:mlbHRPerABSummaryTable) que o desvio padrão varia um pouco de um grupo para o outro. Se essas diferenças são de variação natural não é claro, então devemos relatar essa incerteza com os resultados finais.
<div class="alert alert-info">
<strong>Diagnóstico para uma análise ANOVA</strong>: Independência é sempre importante para uma análise ANOVA. A condição de normalidade é muito importante quando os tamanhos de amostra para cada grupo são relativamente pequenos. A condição de variância constante é especialmente importante quando os tamanhos das amostras diferem entre os grupos.
</div>
### Múltiplas comparações e controle da Taxa de erro Tipo I
Quando rejeitamos a hipótese nula em uma análise ANOVA, poderíamos nos perguntar: quais desses grupos têm diferentes meios? Para responder a essa pergunta, comparamos as médias de cada par de grupos possível. Por exemplo, se existem três grupos e há fortes evidências de que existem algumas diferenças nas médias do grupo, há três comparações a fazer: grupo 1 para grupo 2, grupo 1 para grupo 3 e grupo 2 para grupo 3. As comparações podem ser realizadas usando um teste-$t$ de duas amostras, mas usamos um nível de significância modificado e uma estimativa agrupada do desvio padrão entre os grupos. Normalmente, este desvio padrão agrupado pode ser encontrado na tabela ANOVA, e ao longo do fundo da Tabela \@ref(tab:anovaSummaryTableForOBPAgainstPosition).
```{example}
O Exemplo \@ref(exm:firstExampleForThreeStatisticsClassesAndANOVA) discutimos três classes de estatística, todas ensinadas durante o mesmo semestre. A Tabela \@ref(tab:summaryStatisticsForClassTestData) mostra estatísticas de resumo para esses três cursos, e um gráfico de caixa lado a lado dos dados é mostrado na Figura \@ref(fig:classDataSBSBoxPlot). Gostaríamos de realizar uma ANOVA para esses dados. Você vê algum desvio das três condições para ANOVA?
```
Neste caso (como muitos outros), é difícil verificar a independência de forma rigorosa. Em vez disso, o melhor que podemos fazer é usar o bom senso para considerar as razões pelas quais a suposição de independência pode não ser válida. Por exemplo, a suposição de independência pode não ser razoável se houver um assistente de ensino que somente metade dos alunos possa acessar; tal cenário dividiria uma classe em dois subgrupos. Nenhuma dessas situações foi evidente para esses dados específicos e acreditamos que a independência é aceitável.
As distribuições no gráfico de caixa lado a lado parecem ser mais ou menos simétricas e não mostram outliers perceptíveis. Os gráficos de caixa mostram uma variabilidade aproximadamente igual, que pode ser verificada na Tabela \@ref(tab:summaryStatisticsForClassTestData), apoiando a suposição de variância constante.
```{r summaryStatisticsForClassTestData}
class <- matrix(NA, ncol = 4, nrow = 3)
class[1,] <- c('Tamanho da amostra', 58, 55, 51)
class[2,] <- c('Média da amostra', 75.1, 72.0, 78.9)
class[3,] <- c('DP amostral', 13.9, 13.8, 13.1)
colnames(class) <- c('Classe', 'A', 'B', 'C')
knitr::kable(class, align = "c",
caption = "Estatísticas resumidas para as primeiras pontuações intermediárias em três classes diferentes do mesmo curso.")
```
```{r classDataSBSBoxPlot, fig.cap = 'Gráfico de boxplot para as primeiras pontuações intermediárias em três classes diferentes do mesmo curso.'}
data(classData)
ggplot(data = classData) +
geom_boxplot(aes(x = lecture, y = m1, fill = lecture)) +
theme(legend.position = "none") +
labs(x = 'Classes', y = 'Pontuações') +
theme(panel.border = element_rect(colour = "black", fill=NA, size=1)) +
scale_fill_manual(values = c('#E6205F', '#E97C31', "#EAB217"))
```
***
```{exercise, label = 'exerExaminingAnovaSummaryTableForMidtermData'}
Uma ANOVA foi conduzida para os dados intermediários, e os resultados resumidos são mostrados na Tabela \@ref(tab:anovaSummaryTableForMidtermData). O que devemos concluir?^[O p-valor do teste é 0,0330, menor que o nível de significância padrão de 0,05. Portanto, rejeitamos a hipótese nula e concluímos que a diferença nos escores médios a médio prazo não se deve ao acaso.]
```
***
```{r}
aj <- anova(lm(m1 ~ lecture, classData))
knitr::kable(aj, align = "c", caption = 'Tabela resumo ANOVA para os dados de exames parciais.')
```
Há fortes evidências de que as diferentes médias em cada uma das três classes não se devem simplesmente ao acaso. Podemos nos perguntar, quais das classes são realmente diferentes? Como discutido nos capítulos anteriores, um teste-$t$ de duas amostras poderia ser usado para testar as diferenças em cada par de grupos possível. No entanto, uma armadilha foi discutida no Exemplo \@ref(exm:multipleComparisonExampleThatIncludesDiscussionOfClassrooms): quando executamos tantos testes, a taxa de erro tipo~1 aumenta. Esse problema é resolvido usando um nível de significância modificado.
<div class="alert alert-info">
<strong>Comparações múltiplas e a correção de Bonferroni para $\alpha$</strong>: O cenário de testar muitos pares de grupos é chamado __comparações múltiplas__. A __Correção de Bonferroni__ sugere que um nível de significância mais rigoroso é mais apropriado para esses testes:
\begin{align*}
\alpha^* = \alpha / K
\end{align*}
onde $K$ é o número de comparações consideradas (formal ou informalmente). Se existem $k$ grupos, então todos os pares possíveis são comparados e $K=\frac{k(k-1)}{2}$.
</div>
```{example, label = 'multipleComparisonsOfThreeStatClasses'}
Na Prática Orientada \@ref(exr:exerExaminingAnovaSummaryTableForMidtermData), você encontrou fortes evidências de diferenças nas notas médias entre as três classes. Complete as três possíveis comparações entre pares usando a correção de Bonferroni e relate quaisquer diferenças.
```
Nós usamos um nível de significância modificado de $\alpha^* = 0.05/3 = 0.0167$. Além disso, usamos a estimativa combinada do desvio padrão: $s_{agrupados}=13.61$ com $GL=161$, que é fornecido na tabela de resumo ANOVA.
Aula A versus Aula B: A diferença estimada e o erro padrão são, respectivamente,
\begin{align*}
\bar{x}_A - \bar{x}_{B} &= 75.1 - 72 = 3.1
&EP = \sqrt{\frac{13.61^2}{58} + \frac{13.61^2}{55}} &= 2.56
\end{align*}
Isso resulta em um $T$-escore de 1.21 com $GL = 161$ (usamos o $GL$ associado a $s_{agrupado}$). O software estatístico foi utilizado para identificar precisamente o valor-p de dois lados, uma vez que a significância modificada de 0,0167 não é encontrada na tabela-$t$. O valor-p (0,228) é maior que $\alpha^*=0.0167$, então não há forte evidência de diferença nas médias das classes A e B.
Aula A versus Aula C: A diferença estimada e o erro padrão são 3,8 e 2,61, respectivamente. Isso resulta em uma pontuação de $T$ de 1,46 com $GL=161$ e um valor-p de dois lados de 0,1462. Este valor-p é maior que $\alpha^*$, então não há evidência forte de uma diferença nas médias das classes A e C.
Aula B versus Aula C: A diferença estimada e erro padrão são 6,9 e 2,65, respectivamente. Isso resultando em uma pontuação de $T$ de 2,60 com $GL= 161$ e um valor p de dois lados de 0,0102. Este valor-p é menor que $\alpha^*$. Aqui encontramos fortes evidências de uma diferença nas médias das classes B e C.
Podemos resumir as descobertas da análise de Exemplo \@ref(exm:multipleComparisonsOfThreeStatClasses) usando a seguinte notação:
\begin{align*}
\mu_A &\stackrel{?}{=} \mu_B
&\mu_A &\stackrel{?}{=} \mu_C
&\mu_B &\neq \mu_C
\end{align*}
A média do exame parcial na aula A não é estatisticamente distinta daquelas das aulas B ou C. No entanto, há fortes evidências de que as aulas B e C são diferentes. Nas duas primeiras comparações entre pares, não tivemos evidência suficiente para rejeitar a hipótese nula. Lembre-se de que não rejeitar $H_0$ não implica $H_0$ ser verdade.
<div class="alert alert-info">
<strong>Às vezes, uma ANOVA rejeitará a nula, mas nenhum grupo terá diferenças estatisticamente significativas</strong>: É possível rejeitar a hipótese nula usando ANOVA e depois não identificar diferenças nas comparações pareadas. Contudo, _isso não invalida a conclusão da ANOVA_. Significa apenas que não conseguimos identificar com sucesso quais grupos diferem em suas médias.
</div>
O procedimento ANOVA examina o quadro geral: considera todos os grupos simultaneamente para decifrar se há evidência de que existe alguma diferença. Mesmo que o teste indique que há fortes evidências de diferenças nas médias dos grupos, identificar com alta confiança uma diferença específica como estatisticamente significativa é mais difícil.
Considere a seguinte analogia: observamos uma empresa de Wall Street que faz grandes quantias de dinheiro com base na previsão de fusões. Fusões são geralmente difíceis de prever, e se a taxa de sucesso da previsão é extremamente alta, isso pode ser considerado evidência suficientemente forte para justificar a investigação pela Comissão de Valores Mobiliários e Câmbio (CVM). Embora a CVM possa ter certeza de que há negociações com base em informações privilegiadas ocorrendo na empresa, as evidências contra um único operador podem não ser muito fortes. É somente quando a CVM considera todos os dados que identificam o padrão. Esta é efetivamente a estratégia da ANOVA: recuar e considerar todos os grupos simultaneamente.
install.packages("bookdown")
install.packages("bookdown")
library(bookdown)
ggplot(data = data, aes(x = data$n, y = data$p)) +
geom_line(color = 'darkorange2')+scale_x_log10()+
geom_hline(yintercept = c(0, 1), linetype = 2, color = 'darkblue')
require(ggplot1)
require(ggplot2)
ggplot(data = data, aes(x = data$n, y = data$p)) +
geom_line(color = 'darkorange2')+scale_x_log10()+
geom_hline(yintercept = c(0, 1), linetype = 2, color = 'darkblue')
ggplot(data = data, aes(x = data$n, y = data$p)) +
geom_line(color = 'darkorange2')+scale_x_log10()+
geom_hline(yintercept = c(0, 1), linetype = 2, color = 'darkblue') +
labs(x = "Tamanho de Amostra por grupo", y = "Poder")
n <- c(10:500, seq(510, 2000, 10), seq(2100, 10000, 100))
se <- sapply(n, function(x) sqrt(2 * 12^2 / x))
left.reject <- -1.96 * se
x <- (left.reject - (-3)) / se
p <- pt(x, 2 * n - 2)
ggplot(data = data, aes(x = n, y = p)) +
geom_line(color = 'darkorange2')+scale_x_log10()+
geom_hline(yintercept = c(0, 1), linetype = 2, color = 'darkblue') +
labs(x = "Tamanho de Amostra por grupo", y = "Poder")
require(ggplot2)
n <- c(10:500, seq(510, 2000, 10), seq(2100, 10000, 100))
se <- sapply(n, function(x) sqrt(2 * 12^2 / x))
left.reject <- -1.96 * se
x <- (left.reject - (-3)) / se
p <- pt(x, 2 * n - 2)
ggplot(data = data, aes(x = n, y = p)) +
geom_line(color = 'darkorange2')+scale_x_log10()+
geom_hline(yintercept = c(0, 1), linetype = 2, color = 'darkblue') +
labs(x = "Tamanho de Amostra por grupo", y = "Poder")
n
p
ggplot(data = data, aes(x = n, y = p)) +
geom_line(color = 'darkorange2')+scale_x_log10()+
geom_hline(yintercept = c(0, 1), linetype = 2, color = 'darkblue') +
labs(x = "Tamanho de Amostra por grupo", y = "Poder")
ggplot(aes(x = n, y = p)) +
geom_line(color = 'darkorange2')+scale_x_log10()+
geom_hline(yintercept = c(0, 1), linetype = 2, color = 'darkblue') +
labs(x = "Tamanho de Amostra por grupo", y = "Poder")
ggplot(data = data.frame(n,p), aes(x = n, y = p)) +
geom_line(color = 'darkorange2')+scale_x_log10()+
geom_hline(yintercept = c(0, 1), linetype = 2, color = 'darkblue') +
labs(x = "Tamanho de Amostra por grupo", y = "Poder")
ggplot(data = data.frame(n,p), aes(x = n, y = p)) +
geom_line(color = '#E6205F') + scale_x_log10() +
geom_hline(yintercept = c(0, 1), linetype = 2, size = 1) +
labs(x = "Tamanho de Amostra por grupo", y = "Poder")
ggplot(data = data.frame(n,p), aes(x = n, y = p)) +
geom_line(color = '#E6205F', size = 1) + scale_x_log10() +
geom_hline(yintercept = c(0, 1), linetype = 2) +
labs(x = "Tamanho de Amostra por grupo", y = "Poder")
ggplot(data = data.frame(n,p), aes(x = n, y = p)) +
scale_x_log10() +
geom_line(color = '#E6205F', size = 1) +
geom_hline(yintercept = c(0, 1), linetype = 2) +
labs(x = "Tamanho de Amostra por grupo", y = "Poder") +
theme(panel.border = element_rect(colour = "black", fill=NA, size=1))
bookdown::render_book()
warnings()
